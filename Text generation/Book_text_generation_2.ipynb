{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using BPE."
      ],
      "metadata": {
        "id": "1IWQuC00uJ5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Additional imports for BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ],
      "metadata": {
        "id": "0aFWC-1X8MiJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_clean_text(text):\n",
        "    # Step 1: Find the actual start of the content (e.g., \"Chapter 1\")\n",
        "    start_marker = \"BOOK ONE: 1805\"\n",
        "    start = text.lower().find(start_marker.lower())\n",
        "    if start != -1:\n",
        "        text = text[start:]\n",
        "\n",
        "    # Step 2: Remove URLs and metadata\n",
        "    text = ' '.join([word for word in text.split() if not word.startswith('http')])\n",
        "\n",
        "    # Step 3: Remove special formatting markers and transcriber notes\n",
        "    text = text.replace('_', '')  # Remove underscores\n",
        "    text = text.replace('^', '')  # Remove caret markers\n",
        "    text = text.replace('{', '').replace('}', '')  # Remove curly brackets\n",
        "\n",
        "    # Step 4: Remove punctuation, numbers, and convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])  # Remove numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Step 5: Remove extra spaces\n",
        "    text = ' '.join(text.split())  # Remove redundant spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load raw text file\n",
        "with open(\"War and Peace.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Apply the cleaning function\n",
        "cleaned_text = enhanced_clean_text(raw_text)\n",
        "\n",
        "# Output the first 500 characters of the cleaned text\n",
        "print(cleaned_text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eeKtiU7wA-x",
        "outputId": "88852fee-ef44-4b6d-81b9-35abd3c1ab68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "war and peace by leo tolstoytolstoi contents book one chapter i chapter ii chapter iii chapter iv chapter v chapter vi chapter vii chapter viii chapter ix chapter x chapter xi chapter xii chapter xiii chapter xiv chapter xv chapter xvi chapter xvii chapter xviii chapter xix chapter xx chapter xxi chapter xxii chapter xxiii chapter xxiv chapter xxv chapter xxvi chapter xxvii chapter xxviii book two chapter i chapter ii chapter iii chapter iv chapter v chapter vi chapter vii chapter viii chapter i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=25000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train_from_iterator([cleaned_text], trainer)\n",
        "\n",
        "# Tokenize the text\n",
        "encoded = tokenizer.encode(cleaned_text)\n",
        "text_as_int = np.array(encoded.ids)\n",
        "\n",
        "# Define sequence length and prepare input-output pairs\n",
        "sequence_length = 100\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(text_as_int) - sequence_length):\n",
        "    sequences.append(text_as_int[i:i + sequence_length])\n",
        "    targets.append(text_as_int[i + sequence_length])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ],
      "metadata": {
        "id": "YRgTAX1swMjV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buAWMHlewtHs",
        "outputId": "8b045d1d-3b0c-4ab4-9543-6f1fc7e02b3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.9 * len(sequences))\n",
        "val_size = len(sequences) - train_size\n",
        "\n",
        "train_dataset = TextDataset(sequences[:train_size], targets[:train_size])\n",
        "val_dataset = TextDataset(sequences[train_size:], targets[train_size:])"
      ],
      "metadata": {
        "id": "TWWJAlfSwfb-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Train the given model and visualize training and validation loss over epochs.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        train_loader: DataLoader for training data.\n",
        "        val_loader: DataLoader for validation data.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimizer for training.\n",
        "        num_epochs: Number of training epochs.\n",
        "        device: Device to train on (e.g., \"cpu\" or \"cuda\").\n",
        "\n",
        "    Returns:\n",
        "        model: Trained model.\n",
        "        best_model_state_dict: State dict of the best model based on validation loss.\n",
        "        train_losses: List of training losses for each epoch.\n",
        "        val_losses: List of validation losses for each epoch.\n",
        "    \"\"\"\n",
        "    import copy\n",
        "    # Move model to the device\n",
        "    model.to(device)\n",
        "\n",
        "    # Calculate the number of trainable parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total number of trainable parameters in the model: {total_params}\")\n",
        "\n",
        "    # Lists to store losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Initialize the best validation loss to a large value\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state_dict = None\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs, _ = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Calculate average losses for the epoch\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Store losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Check if this is the best validation loss so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save the final trained model\n",
        "    model_path = \"lstm_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim(0)  # Start y-axis from 0\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model, best_model_state_dict, train_losses, val_losses"
      ],
      "metadata": {
        "id": "ZQYRObcNwD_t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, tokenizer, seed_text, length, device):\n",
        "    model.eval()\n",
        "    generated_ids = tokenizer.encode(seed_text).ids\n",
        "    input_ids = torch.tensor(generated_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_ids, hidden)\n",
        "            probs = torch.nn.functional.softmax(output, dim=-1)\n",
        "            # Sample from the distribution or take the argmax\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        # Append the predicted token id to the generated_ids\n",
        "        generated_ids.append(next_token_id)\n",
        "\n",
        "        # Update input_ids to contain the new token\n",
        "        input_ids = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Decode the generated ids to text\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "K9AitlGlPsTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xaFrxJRLPxfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0-CTl-3jPtJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)"
      ],
      "metadata": {
        "id": "CFZpi5NywWXd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF5MYfJ2xUIi",
        "outputId": "c7c33d71-a888-4d12-8e5a-d40ac02b1181"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1050"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0):\n",
        "        super(LSTMModel2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size,\n",
        "            hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout after LSTM output\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)  # Convert input to embeddings\n",
        "        output, hidden = self.lstm(x, hidden)  # Pass through LSTM layers\n",
        "        output = self.dropout(output)  # Apply dropout to LSTM outputs\n",
        "        output = self.fc(output[:, -1, :])  # Use the last output for prediction\n",
        "        return output, hidden\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "dropout_rate = 0\n",
        "\n",
        "model2_1 = LSTMModel(vocab_size, embed_size, hidden_size, num_layers=num_layers, dropout=dropout_rate)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer2_1 = optim.Adam(model2_1.parameters(), lr=0.001)\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "trained_model2_1, best_model_state_dict2_1, train_losses, val_losses = train_model(\n",
        "    model2_1, train_loader, val_loader, criterion, optimizer2_1, num_epochs=20, device=device\n",
        ")"
      ],
      "metadata": {
        "id": "3m3NoJF1wVCw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models folder if it doesn't exist\n",
        "models_folder = \"models\"\n",
        "os.makedirs(models_folder, exist_ok=True)\n",
        "\n",
        "# Save the last epoch model\n",
        "last_epoch_model_path = os.path.join(models_folder, \"lstm_model2_1.pth\")\n",
        "torch.save(trained_model2_1.state_dict(), last_epoch_model_path)\n",
        "print(f\"Last epoch model saved to {last_epoch_model_path}\")\n",
        "\n",
        "# Save the best model based on validation loss\n",
        "best_model_path = os.path.join(models_folder, \"lstm_model2_1_best.pth\")\n",
        "torch.save(best_model_state_dict2_1, best_model_path)\n",
        "print(f\"Best validation loss model saved to {best_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "BFwSuoLpwbpI",
        "outputId": "63201b5a-f200-4b0b-bfa7-76c576a69893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters in the model: 11927312\n",
            "Epoch 1/20, Train Loss: 6.6545, Val Loss: 6.4536\n",
            "Epoch 2/20, Train Loss: 5.8955, Val Loss: 6.0819\n",
            "Epoch 3/20, Train Loss: 5.5284, Val Loss: 5.9421\n",
            "Epoch 4/20, Train Loss: 5.2692, Val Loss: 5.8660\n",
            "Epoch 5/20, Train Loss: 5.0452, Val Loss: 5.8466\n",
            "Epoch 6/20, Train Loss: 4.8343, Val Loss: 5.8998\n",
            "Epoch 7/20, Train Loss: 4.6269, Val Loss: 5.9612\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-83f512ba469e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a5765bc44059>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Validation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the model\n",
        "def load_model(model_class, model_path, vocab_size, embed_size, hidden_size, num_layers, dropout_rate, device):\n",
        "    model = model_class(vocab_size, embed_size, hidden_size, num_layers=num_layers, dropout=dropout_rate)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the best model\n",
        "model_path = os.path.join(models_folder, \"lstm_model2_1_best.pth\")\n",
        "model2_1_loaded = load_model(LSTMModel2, model_path, vocab_size, embed_size, hidden_size, num_layers, dropout_rate, device)\n",
        "\n",
        "# Generate text using the loaded model\n",
        "seed_text = \"The soldiers marched forward and at a\".lower()  # Convert to lowercase\n",
        "generated_text = generate_text(model2_1_loaded, tokenizer, seed_text, length=200, device=device)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "1P8vHNgu152G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate perplexity on the validation set\n",
        "val_perplexity = calculate_perplexity(model4, val_loader)\n",
        "print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n",
        "\n",
        "entropy = calculate_entropy(generated_text)\n",
        "print(f\"Entropy of Generated Text: {entropy:.2f}\")"
      ],
      "metadata": {
        "id": "RO_dJLBCP1QL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}