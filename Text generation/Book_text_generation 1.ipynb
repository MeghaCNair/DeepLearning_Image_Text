{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def enhanced_clean_text(text):\n",
        "    # Step 1: Find the actual start of the content (e.g., \"Chapter 1\")\n",
        "    start_marker = \"chapter I\"\n",
        "    start = text.lower().find(start_marker)  # Locate the start of \"Chapter 1\"\n",
        "    if start != -1:\n",
        "        text = text[start:]  # Remove everything before \"Chapter 1\"\n",
        "\n",
        "    # Step 2: Remove URLs and metadata\n",
        "    text = ' '.join([word for word in text.split() if not word.startswith('http')])\n",
        "\n",
        "    # Step 3: Remove special formatting markers and transcriber notes\n",
        "    text = text.replace('_', '')  # Remove underscores\n",
        "    text = text.replace('^', '')  # Remove carat markers\n",
        "    text = text.replace('{', '').replace('}', '')  # Remove curly brackets\n",
        "\n",
        "    # Step 4: Remove punctuation, numbers, and convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])  # Remove numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Step 5: Remove extra spaces\n",
        "    text = ' '.join(text.split())  # Remove redundant spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load raw text file\n",
        "with open(\"42671-0.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Apply the cleaning function\n",
        "cleaned_text = enhanced_clean_text(raw_text)\n",
        "\n",
        "# Output the first 500 characters of the cleaned text\n",
        "print(cleaned_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aFWC-1X8MiJ",
        "outputId": "2534031c-7805-4bef-e878-822f5e30f919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start of the project gutenberg ebook note project gutenberg also has an html version of this file which includes the original illustrations see hhtm or hzip httpwwwgutenbergorgfileshhhtm or httpwwwgutenbergorgfileshzip images of the original pages are available through internet archive see transcribers note text enclosed by underscores is in italics italics a carat character is used to denote superscription multiple superscripted characters are enclosed by curly brackets example mrs pride and pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tokenize the text at character level\n",
        "chars = sorted(set(cleaned_text))  # Unique characters\n",
        "char_to_index = {char: idx for idx, char in enumerate(chars)}  # Character to integer mapping\n",
        "index_to_char = {idx: char for idx, char in enumerate(chars)}  # Integer to character mapping\n",
        "\n",
        "# Convert text into integers\n",
        "text_as_int = np.array([char_to_index[char] for char in cleaned_text])\n",
        "\n",
        "# Define sequence length and prepare input-output pairs\n",
        "sequence_length = 100\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(text_as_int) - sequence_length):\n",
        "    sequences.append(text_as_int[i:i+sequence_length])\n",
        "    targets.append(text_as_int[i+sequence_length])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ],
      "metadata": {
        "id": "g-l5rK2f8T7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.9 * len(sequences))\n",
        "val_size = len(sequences) - train_size\n",
        "\n",
        "train_dataset = TextDataset(sequences[:train_size], targets[:train_size])\n",
        "val_dataset = TextDataset(sequences[train_size:], targets[train_size:])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "5W62-uBP9akt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)  # Convert input to embeddings\n",
        "        output, hidden = self.lstm(x, hidden)  # Pass through LSTM\n",
        "        output = self.fc(output[:, -1, :])  # Use the last output for prediction\n",
        "        return output, hidden\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(chars)\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size)"
      ],
      "metadata": {
        "id": "pCVDHOzW9chc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "M9DG86Q29ggI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKl8WVba9iwp",
        "outputId": "a33a8b9c-ac50-4b60-85da-31961352521c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.5747, Val Loss: 1.3525\n",
            "Epoch 2/20, Train Loss: 1.2992, Val Loss: 1.2610\n",
            "Epoch 3/20, Train Loss: 1.2326, Val Loss: 1.2205\n",
            "Epoch 4/20, Train Loss: 1.1966, Val Loss: 1.2061\n",
            "Epoch 5/20, Train Loss: 1.1737, Val Loss: 1.1826\n",
            "Epoch 6/20, Train Loss: 1.1560, Val Loss: 1.1804\n",
            "Epoch 7/20, Train Loss: 1.1438, Val Loss: 1.1718\n",
            "Epoch 8/20, Train Loss: 1.1336, Val Loss: 1.1653\n",
            "Epoch 9/20, Train Loss: 1.1249, Val Loss: 1.1600\n",
            "Epoch 10/20, Train Loss: 1.1173, Val Loss: 1.1523\n",
            "Epoch 11/20, Train Loss: 1.1107, Val Loss: 1.1552\n",
            "Epoch 12/20, Train Loss: 1.1054, Val Loss: 1.1534\n",
            "Epoch 13/20, Train Loss: 1.0998, Val Loss: 1.1486\n",
            "Epoch 14/20, Train Loss: 1.0956, Val Loss: 1.1530\n",
            "Epoch 15/20, Train Loss: 1.0926, Val Loss: 1.1446\n",
            "Epoch 16/20, Train Loss: 1.0887, Val Loss: 1.1435\n",
            "Epoch 17/20, Train Loss: 1.0853, Val Loss: 1.1478\n",
            "Epoch 18/20, Train Loss: 1.0833, Val Loss: 1.1403\n",
            "Epoch 19/20, Train Loss: 1.0802, Val Loss: 1.1447\n",
            "Epoch 20/20, Train Loss: 1.0782, Val Loss: 1.1452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, length):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    generated_text = seed_text\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Convert seed text to integers\n",
        "        input_seq = torch.tensor([char_to_index[char] for char in seed_text[-sequence_length:]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)  # Forward pass\n",
        "            output = output.squeeze(0)  # Remove batch dimension\n",
        "            next_char_index = torch.argmax(output).item()  # Select the most probable character deterministically\n",
        "\n",
        "        # Append the predicted character\n",
        "        next_char = index_to_char[next_char_index]\n",
        "        generated_text += next_char\n",
        "        seed_text += next_char\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate example text\n",
        "seed_text = \"It is a truth universally acknowledged that\".lower()  # Convert to lowercase\n",
        "generated_text = generate_text(model, seed_text, length=200)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GrYHP3Q9sXR",
        "outputId": "866efd3e-4854-47e2-bd88-5ccb474853a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it is a truth universally acknowledged that he had not to the companion of her family and the common and the common and the common and the common and the common and the common and the common and the common and the common and the common and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function used for training\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, _ = model(inputs)  # Get model predictions\n",
        "            loss = criterion(outputs, targets)  # Compute loss\n",
        "            total_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "            total_count += inputs.size(0)  # Count tokens\n",
        "\n",
        "    # Calculate perplexity\n",
        "    average_loss = total_loss / total_count\n",
        "    perplexity = math.exp(average_loss)\n",
        "    return perplexity\n",
        "\n",
        "# Evaluate perplexity on the validation set\n",
        "val_perplexity = calculate_perplexity(model, val_loader)\n",
        "print(f\"Validation Perplexity: {val_perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5aGta4G-ZFm",
        "outputId": "d53c9321-4711-43e7-ca4c-6d7dcd09d68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Perplexity: 3.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_entropy(generated_text):\n",
        "    # Count the frequency of each character\n",
        "    char_counts = Counter(generated_text)\n",
        "    total_chars = sum(char_counts.values())\n",
        "\n",
        "    # Compute entropy\n",
        "    entropy = -sum((count / total_chars) * math.log2(count / total_chars) for count in char_counts.values())\n",
        "    return entropy\n",
        "\n",
        "# Example usage\n",
        "entropy = calculate_entropy(generated_text)\n",
        "print(f\"Entropy of Generated Text: {entropy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BUmiY2R-47L",
        "outputId": "12ed2542-b794-4240-9c8b-e6592bf750af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of Generated Text: 3.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Example reference and candidate texts\n",
        "reference_text = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\".split()\n",
        "candidate_text = generated_text.lower().split()\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_score = sentence_bleu([reference_text], candidate_text)\n",
        "print(f\"BLEU Score: {bleu_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaf-v9K-_JlI",
        "outputId": "86547c92-58cf-4e43-e5ea-46807236c6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.12\n"
          ]
        }
      ]
    }
  ]
}